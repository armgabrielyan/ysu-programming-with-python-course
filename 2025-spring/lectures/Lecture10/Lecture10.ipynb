{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a9302f",
   "metadata": {},
   "source": [
    "# Programming with Python\n",
    "\n",
    "## Lecture 10: Concurrency 2\n",
    "\n",
    "### Armen Gabrielyan\n",
    "\n",
    "#### Yerevan State University / ASDS\n",
    "\n",
    "#### 26 Apr, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3595c4c",
   "metadata": {},
   "source": [
    "This section is heavily influenced by the following:\n",
    "\n",
    "*References:*\n",
    "\n",
    "- Fluent Python, Luciano Ramalho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7301a",
   "metadata": {},
   "source": [
    "### Race condition\n",
    "\n",
    "A **race condition** happens when two or more threads access shared data at the same time, and the result depends on the order of execution — which is not predictable.\n",
    "\n",
    "Think of two people writing on the same paper at the same time without coordinating. You could end up with gibberish.\n",
    "\n",
    "It's a problem because threads may:\n",
    "\n",
    "- Read stale or incorrect values\n",
    "- Overwrite each other’s work\n",
    "- Cause inconsistent or unexpected results\n",
    "\n",
    "**See practical example 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02221901",
   "metadata": {},
   "source": [
    "### Synchronization\n",
    "\n",
    "**Synchronization** is the key to managing shared resources in multi-threading. It is a concept that specifies various mechanisms to ensure that no more than one concurrent thread/process can process and execute a particular program portion at a time; this portion is known as the **critical section**. Synchronization ensures that only one thread at a time can access critical sections of code or shared data, preventing race conditions and inconsistent results.\n",
    "\n",
    "In a given program, when a thread is accessing/executing the critical section of the program, the other threads have to wait until that thread finishes executing. The typical goal of thread synchronization is to avoid any potential data discrepancies / race conditions when multiple threads access their shared resources, **allowing only one thread to execute the critical section of the program at a time** guarantees that no data conflicts occur in multithreaded applications.\n",
    "\n",
    "Let's discuss some of the synchronization mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449f594",
   "metadata": {},
   "source": [
    "#### 1. Lock / mutual exclusion (mutex)\n",
    "\n",
    "One of the most common ways to apply thread synchronization is through the implementation of a locking mechanism. In the `threading` module, the `threading.Lock` class provides a simple and intuitive approach to creating and working with locks. Its main usage includes the following methods: \n",
    "\n",
    "- `threading.Lock()`: This method initializes and returns a new lock object.\n",
    "- `acquire(blocking)`: When this method is called, all of the threads will run synchronously (that is, only one thread can execute the critical section at a time). The optional argument blocking allows us to specify whether the current thread should wait to acquire the lock:\n",
    "  - When `blocking = 0`, the current thread does not wait for the lock and simply returns 0 if the lock cannot be acquired by the thread, or 1 otherwise\n",
    "  - When `blocking = 1` (default value), the current thread blocks and waits for the lock to be released and acquires it afterwards\n",
    "- `release()`: When this method is called, the lock is released.\n",
    "\n",
    "Common pattern:\n",
    "\n",
    "```python\n",
    "import threading\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "try:\n",
    "    # Acquire the lock\n",
    "    lock.acquire()\n",
    "    \n",
    "    # Critical section - only one thread at a time can execute this code\n",
    "    critical_section_code()\n",
    "finally:\n",
    "    # Always release the lock, even if an exception occurs\n",
    "    lock.release()\n",
    "```\n",
    "\n",
    "Lock implements context manager protocol, so it is better practice to use `with` statement:\n",
    "\n",
    "```python\n",
    "import threading\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "with lock: # Automatically acquires and releases the lock \n",
    "    critical_region_code() # Critical section - only one thread at a time can execute this code\n",
    "```\n",
    "\n",
    "**See practical example 2**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3aef9d",
   "metadata": {},
   "source": [
    "#### 2. Semaphore\n",
    "\n",
    "This is one of the oldest synchronization primitives in the history of computer science, invented by the early Dutch computer scientist Edsger W. Dijkstra.\n",
    "\n",
    "A semaphore manages an internal counter which is decremented by each `acquire()` call and incremented by each `release()` call. The counter can never go below zero; when `acquire()` finds that it is zero, it blocks, waiting until some other thread calls `release()`.\n",
    "\n",
    "Use cases:\n",
    "\n",
    "- **Resource pool**: Allow only 5 simultaneous DB connections\n",
    "- **Rate limiting**: Max 2 API calls at once\n",
    "- **Thread-safe batching**: Limit how many threads can download files at once\n",
    "\n",
    "The `threading` module provides `threading.Semaphore` class for managing semaphores.\n",
    "\n",
    "**See practical example 3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd3079f",
   "metadata": {},
   "source": [
    "#### 3. Event\n",
    "\n",
    "Used to signal between threads — one thread waits, another signals.\n",
    "\n",
    "An event object manages an internal flag that can be set to true with the `set()` method and reset to false with the `clear()` method. The `wait()` method blocks until the flag is true.\n",
    "\n",
    "The `threading` module provides `threading.Event`.\n",
    "\n",
    "**See practical example 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d54ba",
   "metadata": {},
   "source": [
    "#### 4. Queue\n",
    "\n",
    "A concept in computer science that is widely used in concurrent programming is queuing. **Queue** is a data structure that is a collection of different elements. Elements can be added to the end of the queue which is called enqueuing. Elements can be removed from the beginning of the queue, called dequeuing. It works in First in First out (FIFO) manner, meaning that first entered element is removed first. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/52/Data_Queue.svg/1200px-Data_Queue.svg.png\" alt=\"Index\" width=\"400\" height=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "The `queue` module in Python provides a simple implementation of the queue data structure. Each queue in the `queue.Queue` class can hold a specific amount of elements, and can have the following methods as its high-level API:\n",
    "- `get()`: This method returns the next element of the calling queue object and removes it from the queue object\n",
    "- `put()`: This method adds a new element to the calling queue object \n",
    "- `qsize()`: This method returns the number of current elements in the calling queue object (that is, its size)\n",
    "- `empty()`: This method returns a Boolean, indicating whether the calling queue object is empty\n",
    "- `full()`: This method returns a Boolean, indicating whether the calling queue object is full\n",
    "\n",
    "Sometimes it is undesirable to have as many threads as the tasks we have to process. Say we have a large number of tasks to be processed, then it will be quite inefficient to spawn the same large number of threads and have each thread execute only one task. It could be more beneficial to have a **fixed number of threads (commonly known as a thread pool)** that would work through the tasks in a cooperative manner.\n",
    "\n",
    "Here is when the concept of a queue comes in. We can design a structure in which the pool of threads will not hold any information regarding the tasks they should each execute, instead the tasks are stored in a queue (in other words task queue), and the items in the queue will be fed to individual members of the thread pool. As a given task is completed by a member of the thread pool, if the task queue still contains elements to be processed, then the next element in the queue will be sent to the thread that just became available.\n",
    "\n",
    "**See practical example 5**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4cac8",
   "metadata": {},
   "source": [
    "## CPU-intensive task\n",
    "\n",
    "In Python, CPU-intensive tasks are best handled with `multiprocessing` rather than `threading`, mainly due to the limitations of the Global Interpreter Lock (GIL).\n",
    "\n",
    "The GIL ensures that only one thread executes Python bytecode at a time, even on multi-core processors. This means that:\n",
    "\n",
    "- Threading does not provide real parallelism for CPU-bound tasks.\n",
    "- Threads still take turns using the CPU, resulting in limited performance gain or even overhead from context switching.\n",
    "- In contrast, multiprocessing creates separate processes, each with its own Python interpreter and memory space, allowing for true parallel execution across multiple CPU cores.\n",
    "\n",
    "Key points\n",
    "\n",
    "- Multiprocessing bypasses the GIL, enabling full CPU core usage.\n",
    "- Threading is limited by the GIL for CPU-bound work.\n",
    "- Multiprocessing is ideal for tasks like number crunching, image processing, or simulations.\n",
    "- Threading is better suited for I/O-bound tasks (e.g., file reads, network requests).\n",
    "\n",
    "In summary, due to the GIL, `threading` is ineffective for CPU-heavy workloads, whereas `multiprocessing` provides actual parallelism and improved performance.\n",
    "\n",
    "Let's see this in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd81c6e",
   "metadata": {},
   "source": [
    "## Prime number checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e4f11f",
   "metadata": {},
   "source": [
    "### Sequential\n",
    "\n",
    "**See practical example 6**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2dbfe0",
   "metadata": {},
   "source": [
    "### Multi-threading\n",
    "\n",
    "**See practical example 7**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3118f74",
   "metadata": {},
   "source": [
    "### Multi-processing\n",
    "\n",
    "**See practical example 8**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b201a9",
   "metadata": {},
   "source": [
    "## Real-world example: simple web scraper\n",
    "\n",
    "To demonstrate the difference in execution time between sequential and multithreaded approaches, we'll simulate downloading content from multiple URLs.\n",
    "\n",
    "Requesting a content over a network is I/O-bound task and well-suited for multi-threading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6469ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c112b7",
   "metadata": {},
   "source": [
    "### Sequential\n",
    "\n",
    "**See practical example 9**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24426cb9",
   "metadata": {},
   "source": [
    "### Multi-threading\n",
    "\n",
    "**See practical example 10**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995fc553",
   "metadata": {},
   "source": [
    "### Multi-processing\n",
    "\n",
    "We can do it with multi-processing, but since this is a I/O-bound task, it is better to solve the problem with multi-threading. Multi-processing can create additional overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed835b",
   "metadata": {},
   "source": [
    "## Concurrent Executors and Futures\n",
    "\n",
    "Python's `concurrent.futures` module provides a high-level interface for asynchronously executing tasks using threads or processes. It combines clean syntax with powerful concurrent programming capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5898ae3",
   "metadata": {},
   "source": [
    "### Key Components\n",
    "\n",
    "#### Executors\n",
    "\n",
    "The `concurrent.futures` module provides two primary executor classes:\n",
    "\n",
    "1. `ThreadPoolExecutor`: Uses threads for concurrent execution\n",
    "\n",
    "```python\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    # Submit tasks to be executed concurrently\n",
    "```\n",
    "\n",
    "2. `ProcessPoolExecutor`: Uses processes for concurrent execution\n",
    "\n",
    "```python\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    # Submit tasks to be executed concurrently\n",
    "```\n",
    "\n",
    "#### Futures\n",
    "\n",
    "A `Future` represents the result of an asynchronous computation. It's a placeholder for a value that will be available when the computation completes.\n",
    "\n",
    "\n",
    "```python\n",
    "future = executor.submit(function, arg1, arg2)  # Returns immediately\n",
    "\n",
    "# Check if done without blocking\n",
    "if future.done():\n",
    "    print(\"Task completed\")\n",
    "\n",
    "# Get result (will block until ready)\n",
    "result = future.result()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd361b40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
