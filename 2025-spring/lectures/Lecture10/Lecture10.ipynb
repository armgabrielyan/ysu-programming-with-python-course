{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a9302f",
   "metadata": {},
   "source": [
    "# Programming with Python\n",
    "\n",
    "## Lecture 10: Concurrency 2\n",
    "\n",
    "### Armen Gabrielyan\n",
    "\n",
    "#### Yerevan State University / ASDS\n",
    "\n",
    "#### 26 Apr, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b50ba54",
   "metadata": {},
   "source": [
    "# Multi-threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd522815",
   "metadata": {},
   "source": [
    "## `threading` module\n",
    "\n",
    "### Overview\n",
    "\n",
    "`threading` module constructs higher-level threading interfaces on top of the lower level `_thread` module.\n",
    "\n",
    "**CPython implementation detail:** In CPython, due to the Global Interpreter Lock, only one thread can execute Python code at once (even though certain performance-oriented libraries might overcome this limitation). If you want your application to make better use of the computational resources of multi-core machines, you are advised to use `multiprocessing` or `concurrent.futures.ProcessPoolExecutor`. However, threading is still an appropriate model if you want to run multiple I/O-bound tasks simultaneously.\n",
    "\n",
    "Here are the primary methods and attributes defined on a `threading.Thread` object:\n",
    "\n",
    "| Method / Attribute      | Signature                         | Description                                                                                                                                                   |\n",
    "|-------------------------|-----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **`__init__`**          | `Thread(group=None, target=None, name=None, args=(), kwargs={}, daemon=None)` | Constructor. You almost always supply `target`, optionally `args`/`kwargs`, and you can name or mark it daemon.                                             |\n",
    "| **`start`**             | `start()`                         | Arrange for the thread’s `run()` method to be invoked in a new thread of control. Returns immediately.                                                        |\n",
    "| **`run`**               | `run()`                           | The “worker” function. By default calls `self._target(*self._args, **self._kwargs)`. You override this in subclasses instead of passing `target`.            |\n",
    "| **`join`**              | `join(timeout=None)`              | Block the calling thread until this thread terminates (or until `timeout` seconds pass).                                                                     |\n",
    "| **`is_alive`**          | `is_alive()`                      | Return `True` if the thread is still executing.                                                                                                              |\n",
    "| **`name`**  | `name` (property)       | Get or set the thread’s name (useful for debugging/logging).                                                                                                  |\n",
    "| **`daemon`**            | `daemon` (property)                   | Boolean flag. If `True`, the thread won’t keep the process alive. Must be set before `start()`.                                                              |\n",
    "| **`ident`**             | `ident` (read‑only)               | Thread identifier (an integer) assigned by the OS upon `start()`, or `None` if not yet started.                                                               |\n",
    "| **`native_id`**         | `native_id` (read‑only)           | Native (kernel-level) thread ID, if supported by the platform.                                                                                 |\n",
    "\n",
    "\n",
    "Also, `threading` module includes module level functions. For example, `threading.current_thread()` returns the current `Thread` object, corresponding to the caller’s thread of control. If the caller’s thread of control was not created through the `threading` module, a dummy thread object with limited functionality is returned.\n",
    "\n",
    "**See practical example 1**.\n",
    "\n",
    "\n",
    "### Overriding `run()` via subclassing\n",
    "\n",
    "Use subclassing when you need per-thread state or more complex behavior; otherwise, passing `target`/`args` to the constructor is often simpler.\n",
    "\n",
    "**See practical example 2**.\n",
    "\n",
    "\n",
    "### Thread-local data\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "In multi-threaded applications, you often encounter situations with variables:\n",
    "\n",
    "1.  **Local variables:** These exist only within a function's scope and are naturally private to the thread executing that function call. However, they disappear when the function returns.\n",
    "2.  **Global variables:** These are accessible by *all* threads within the process. While convenient for sharing data, modifying shared global variables requires careful use of locks (`threading.Lock`) to prevent race conditions, adding complexity.\n",
    "\n",
    "**What if you need data that:**\n",
    "\n",
    "* Persists across function calls *within the same thread* (like a global variable)?\n",
    "* But is *isolated* and *private* to each specific thread (like a local variable)?\n",
    "\n",
    "\n",
    "**The Solution: `threading.local()`**\n",
    "\n",
    "Python's `threading` module offers the `local()` class to create objects that manage thread-local data. Thread-local data is data whose values are thread specific. To manage thread-local data, just create an instance of `threading.local` (or a subclass) and store attributes on it:\n",
    "\n",
    "```python\n",
    "mydata = threading.local()\n",
    "mydata.x = 1\n",
    "```\n",
    "\n",
    "The instance’s values will be different for separate threads.\n",
    "\n",
    "**See practical example 3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7301a",
   "metadata": {},
   "source": [
    "### Race condition\n",
    "\n",
    "A **race condition** happens when two or more threads access shared data at the same time, and the result depends on the order of execution — which is not predictable.\n",
    "\n",
    "Think of two people writing on the same paper at the same time without coordinating. You could end up with gibberish.\n",
    "\n",
    "It's a problem because threads may:\n",
    "\n",
    "- Read stale or incorrect values\n",
    "- Overwrite each other’s work\n",
    "- Cause inconsistent or unexpected results\n",
    "\n",
    "**See practical example 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02221901",
   "metadata": {},
   "source": [
    "### Synchronization\n",
    "\n",
    "**Synchronization** is the key to managing shared resources in multi-threading. It is a concept that specifies various mechanisms to ensure that no more than one concurrent thread/process can process and execute a particular program portion at a time; this portion is known as the **critical section**. Synchronization ensures that only one thread at a time can access critical sections of code or shared data, preventing race conditions and inconsistent results.\n",
    "\n",
    "In a given program, when a thread is accessing/executing the critical section of the program, the other threads have to wait until that thread finishes executing. The typical goal of thread synchronization is to avoid any potential data discrepancies / race conditions when multiple threads access their shared resources, **allowing only one thread to execute the critical section of the program at a time** guarantees that no data conflicts occur in multithreaded applications.\n",
    "\n",
    "Let's discuss some of the synchronization mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449f594",
   "metadata": {},
   "source": [
    "#### 1. Lock / mutual exclusion (mutex)\n",
    "\n",
    "One of the most common ways to apply thread synchronization is through the implementation of a locking mechanism. In the `threading` module, the `threading.Lock` class provides a simple and intuitive approach to creating and working with locks. Its main usage includes the following methods: \n",
    "\n",
    "- `threading.Lock()`: This method initializes and returns a new lock object.\n",
    "- `acquire(blocking)`: When this method is called, all of the threads will run synchronously (that is, only one thread can execute the critical section at a time). The optional argument blocking allows us to specify whether the current thread should wait to acquire the lock:\n",
    "  - When `blocking = 0`, the current thread does not wait for the lock and simply returns 0 if the lock cannot be acquired by the thread, or 1 otherwise\n",
    "  - When `blocking = 1` (default value), the current thread blocks and waits for the lock to be released and acquires it afterwards\n",
    "- `release()`: When this method is called, the lock is released.\n",
    "\n",
    "Common pattern:\n",
    "\n",
    "```python\n",
    "import threading\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "try:\n",
    "    # Acquire the lock\n",
    "    lock.acquire()\n",
    "    \n",
    "    # Critical section - only one thread at a time can execute this code\n",
    "    critical_section_code()\n",
    "finally:\n",
    "    # Always release the lock, even if an exception occurs\n",
    "    lock.release()\n",
    "```\n",
    "\n",
    "Lock implements context manager protocol, so it is better practice to use `with` statement:\n",
    "\n",
    "```python\n",
    "import threading\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "with lock: # Automatically acquires and releases the lock \n",
    "    critical_region_code() # Critical section - only one thread at a time can execute this code\n",
    "```\n",
    "\n",
    "**See practical example 5**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3aef9d",
   "metadata": {},
   "source": [
    "#### 2. Semaphore\n",
    "\n",
    "This is one of the oldest synchronization primitives in the history of computer science, invented by the early Dutch computer scientist Edsger W. Dijkstra.\n",
    "\n",
    "A semaphore manages an internal counter which is decremented by each `acquire()` call and incremented by each `release()` call. The counter can never go below zero; when `acquire()` finds that it is zero, it blocks, waiting until some other thread calls `release()`.\n",
    "\n",
    "Use cases:\n",
    "\n",
    "- **Resource pool**: Allow only 5 simultaneous DB connections\n",
    "- **Rate limiting**: Max 2 API calls at once\n",
    "- **Thread-safe batching**: Limit how many threads can download files at once\n",
    "\n",
    "The `threading` module provides `threading.Semaphore` class for managing semaphores.\n",
    "\n",
    "**See practical example 6**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd3079f",
   "metadata": {},
   "source": [
    "#### 3. Event\n",
    "\n",
    "Used to signal between threads — one thread waits, another signals.\n",
    "\n",
    "An event object manages an internal flag that can be set to true with the `set()` method and reset to false with the `clear()` method. The `wait()` method blocks until the flag is true.\n",
    "\n",
    "The `threading` module provides `threading.Event`.\n",
    "\n",
    "**See practical example 7**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d54ba",
   "metadata": {},
   "source": [
    "#### 4. Queue\n",
    "\n",
    "A concept in computer science that is widely used in concurrent programming is queuing. **Queue** is a data structure that is a collection of different elements. Elements can be added to the end of the queue which is called enqueuing. Elements can be removed from the beginning of the queue, called dequeuing. It works in First in First out (FIFO) manner, meaning that first entered element is removed first. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/52/Data_Queue.svg/1200px-Data_Queue.svg.png\" alt=\"Index\" width=\"400\" height=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "The `queue` module in Python provides a simple implementation of the queue data structure. Each queue in the `queue.Queue` class can hold a specific amount of elements, and can have the following methods as its high-level API:\n",
    "- `get()`: This method returns the next element of the calling queue object and removes it from the queue object. If optional args `block` is `True` (the default) and `timeout` is `None` (the default), block if necessary until an item is available. If `timeout` is a positive number, it blocks at most `timeout` seconds and raises the `queue.Empty` exception if no item was available within that time.\n",
    "- `get_nowait()`: This method is equivalent to `get()` with `block` parameter set to `False`, meaning it returns an item if one is immediately available, else raises the `queue.Empty` exception .\n",
    "- `put()`: This method adds a new element to the calling queue object. Similar to `get()`, it also has `block` and `timeout` args and may raise `queue.Full` exception.\n",
    "- `qsize()`: This method returns the number of current elements in the calling queue object (that is, its size)\n",
    "- `empty()`: This method returns a boolean, indicating whether the calling queue object is empty\n",
    "- `full()`: This method returns a boolean, indicating whether the calling queue object is full\n",
    "- `task_done()`: This method indicates that a formerly enqueued task is complete. Used by queue consumer threads. For each `get()` used to fetch a task, a subsequent call to `task_done()` tells the queue that the processing on the task is complete.\n",
    "- `join()`: This method blocks until all items in the queue have been gotten and processed. The count of unfinished tasks goes up whenever an item is added to the queue. The count goes down whenever a consumer thread calls `task_done()` to indicate that the item was retrieved and all work on it is complete. When the count of unfinished tasks drops to zero, `join()` unblocks.\n",
    "\n",
    "Sometimes it is undesirable to have as many threads as the tasks we have to process. Say we have a large number of tasks to be processed, then it will be quite inefficient to spawn the same large number of threads and have each thread execute only one task. It could be more beneficial to have a **fixed number of threads (commonly known as a thread pool)** that would work through the tasks in a cooperative manner.\n",
    "\n",
    "Here is when the concept of a queue comes in. We can design a structure in which the pool of threads will not hold any information regarding the tasks they should each execute, instead the tasks are stored in a queue (in other words task queue), and the items in the queue will be fed to individual members of the thread pool. As a given task is completed by a member of the thread pool, if the task queue still contains elements to be processed, then the next element in the queue will be sent to the thread that just became available.\n",
    "\n",
    "**See practical example 8**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b775d158",
   "metadata": {},
   "source": [
    "## IO-bound task\n",
    "\n",
    "In Python, I/O‑bound tasks are best handled with `threading` because most I/O operations (disk reads/writes, network calls, database queries) release the GIL while they’re waiting, you can get real concurrency with threads. Here’s how it compares:\n",
    "\n",
    "- **Threading**  \n",
    "  - **GIL release**: Most blocking I/O operations in the standard library (e.g., `socket`, file I/O, many database drivers) relinquish the GIL while waiting, so another thread can run.  \n",
    "  - **Low overhead**: Threads share memory and have much lower start‑up and context‑switch costs than processes.  \n",
    "  - **Ease of sharing state**: Because threads live in the same address space, shared data structures require only locks or queues, not inter-process communication (IPC).  \n",
    "  - **Good for high‑latency tasks**: If your workload is dominated by waiting for network responses, disk reads, or other slow operations, threading can boost throughput without much complexity.\n",
    "\n",
    "- **Multiprocessing**  \n",
    "  - **Still works**: Processes also release the GIL on blocking I/O, but using multiple processes adds IPC overhead and higher memory usage.  \n",
    "  - **Overkill for I/O**: Spinning up separate interpreters usually isn’t worth it if you’re not CPU‑bound; threads are lighter and simpler.\n",
    "\n",
    "- **Asynchronous I/O (async/await)**  \n",
    "  - **Single-threaded concurrency**: Uses an event loop to switch between tasks when they await I/O.  \n",
    "  - **Even lighter than threads**: No context‑switching at the OS level; perfect for handling *huge* numbers of concurrent connections (web servers, chat clients).  \n",
    "  - **Requires non‑blocking APIs**: You need libraries designed for asyncio or third‑party frameworks (e.g., `aiohttp`, `asyncpg`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b201a9",
   "metadata": {},
   "source": [
    "### Simple web scraper\n",
    "\n",
    "To demonstrate the difference in execution time between sequential and multithreaded approaches, we'll simulate downloading content from multiple URLs.\n",
    "\n",
    "Requesting a content over a network is I/O-bound task and well-suited for multi-threading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6469ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c112b7",
   "metadata": {},
   "source": [
    "In the following examples, we use sequential and multi-threading techniques to download websites. Also, we can do it with multi-processing, but since this is a I/O-bound task, it is better to solve the problem with multi-threading. Multi-processing can create additional overhead.\n",
    "\n",
    "**See practical example 9**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c3bb6f",
   "metadata": {},
   "source": [
    "## Python experimental support for free threading\n",
    "\n",
    "Starting with version `3.13`, CPython introduces an experimental feature called free threading, which removes the Global Interpreter Lock (GIL). This change enables true parallel execution of threads across multiple CPU cores, allowing programs that are designed to use threading to take full advantage of multi-core systems and potentially see performance improvements.\n",
    "\n",
    "However, this mode is still in development and may contain bugs. Additionally, programs that run in a single thread may experience a noticeable performance decrease.\n",
    "\n",
    "This was introduced in [PEP 703 – Making the Global Interpreter Lock Optional in CPython](https://peps.python.org/pep-0703/). See [documentation](https://docs.python.org/3/howto/free-threading-python.html) for installation guides and more information.\n",
    "\n",
    "**See practical example 10**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8778d4f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [threading — Thread-based parallelism](https://docs.python.org/3/library/threading.html)\n",
    "- [Python experimental support for free threading](https://docs.python.org/3/howto/free-threading-python.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
