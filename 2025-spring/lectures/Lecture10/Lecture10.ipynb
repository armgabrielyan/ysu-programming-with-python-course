{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a9302f",
   "metadata": {},
   "source": [
    "# Programming with Python\n",
    "\n",
    "## Lecture 10: Concurrency 2\n",
    "\n",
    "### Armen Gabrielyan\n",
    "\n",
    "#### Yerevan State University / ASDS\n",
    "\n",
    "#### 26 Apr, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b50ba54",
   "metadata": {},
   "source": [
    "# Multi-threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd522815",
   "metadata": {},
   "source": [
    "## `threading` module\n",
    "\n",
    "### Overview\n",
    "\n",
    "`threading` module constructs higher-level threading interfaces on top of the lower level `_thread` module.\n",
    "\n",
    "**CPython implementation detail:** In CPython, due to the Global Interpreter Lock, only one thread can execute Python code at once (even though certain performance-oriented libraries might overcome this limitation). If you want your application to make better use of the computational resources of multi-core machines, you are advised to use `multiprocessing` or `concurrent.futures.ProcessPoolExecutor`. However, threading is still an appropriate model if you want to run multiple I/O-bound tasks simultaneously.\n",
    "\n",
    "Here are the primary methods and attributes defined on a `threading.Thread` object:\n",
    "\n",
    "| Method / Attribute      | Signature                         | Description                                                                                                                                                   |\n",
    "|-------------------------|-----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **`__init__`**          | `Thread(group=None, target=None, name=None, args=(), kwargs={}, daemon=None)` | Constructor. You almost always supply `target`, optionally `args`/`kwargs`, and you can name or mark it daemon.                                             |\n",
    "| **`start`**             | `start()`                         | Arrange for the thread’s `run()` method to be invoked in a new thread of control. Returns immediately.                                                        |\n",
    "| **`run`**               | `run()`                           | The “worker” function. By default calls `self._target(*self._args, **self._kwargs)`. You override this in subclasses instead of passing `target`.            |\n",
    "| **`join`**              | `join(timeout=None)`              | Block the calling thread until this thread terminates (or until `timeout` seconds pass).                                                                     |\n",
    "| **`is_alive`**          | `is_alive()`                      | Return `True` if the thread is still executing.                                                                                                              |\n",
    "| **`name`**  | `name` (property)       | Get or set the thread’s name (useful for debugging/logging).                                                                                                  |\n",
    "| **`daemon`**            | `daemon` (property)                   | Boolean flag. If `True`, the thread won’t keep the process alive. Must be set before `start()`.                                                              |\n",
    "| **`ident`**             | `ident` (read‑only)               | Thread identifier (an integer) assigned by the OS upon `start()`, or `None` if not yet started.                                                               |\n",
    "| **`native_id`**         | `native_id` (read‑only)           | Native (kernel-level) thread ID, if supported by the platform.                                                                                 |\n",
    "\n",
    "\n",
    "Also, `threading` module includes module level functions. For example, `threading.current_thread()` returns the current `Thread` object, corresponding to the caller’s thread of control. If the caller’s thread of control was not created through the `threading` module, a dummy thread object with limited functionality is returned.\n",
    "\n",
    "**See practical example 1**.\n",
    "\n",
    "\n",
    "### Overriding `run()` via subclassing\n",
    "\n",
    "Use subclassing when you need per-thread state or more complex behavior; otherwise, passing `target`/`args` to the constructor is often simpler.\n",
    "\n",
    "**See practical example 2**.\n",
    "\n",
    "\n",
    "### Thread-local data\n",
    "\n",
    "**The Problem:**\n",
    "\n",
    "In multi-threaded applications, you often encounter situations with variables:\n",
    "\n",
    "1.  **Local variables:** These exist only within a function's scope and are naturally private to the thread executing that function call. However, they disappear when the function returns.\n",
    "2.  **Global variables:** These are accessible by *all* threads within the process. While convenient for sharing data, modifying shared global variables requires careful use of locks (`threading.Lock`) to prevent race conditions, adding complexity.\n",
    "\n",
    "**What if you need data that:**\n",
    "\n",
    "* Persists across function calls *within the same thread* (like a global variable)?\n",
    "* But is *isolated* and *private* to each specific thread (like a local variable)?\n",
    "\n",
    "\n",
    "**The Solution: `threading.local()`**\n",
    "\n",
    "Python's `threading` module offers the `local()` class to create objects that manage thread-local data. Thread-local data is data whose values are thread specific. To manage thread-local data, just create an instance of `threading.local` (or a subclass) and store attributes on it:\n",
    "\n",
    "```python\n",
    "mydata = threading.local()\n",
    "mydata.x = 1\n",
    "```\n",
    "\n",
    "The instance’s values will be different for separate threads.\n",
    "\n",
    "**See practical example 3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7301a",
   "metadata": {},
   "source": [
    "### Race condition\n",
    "\n",
    "A **race condition** happens when two or more threads access shared data at the same time, and the result depends on the order of execution — which is not predictable.\n",
    "\n",
    "Think of two people writing on the same paper at the same time without coordinating. You could end up with gibberish.\n",
    "\n",
    "It's a problem because threads may:\n",
    "\n",
    "- Read stale or incorrect values\n",
    "- Overwrite each other’s work\n",
    "- Cause inconsistent or unexpected results\n",
    "\n",
    "**See practical example 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02221901",
   "metadata": {},
   "source": [
    "### Synchronization\n",
    "\n",
    "**Synchronization** is the key to managing shared resources in multi-threading. It is a concept that specifies various mechanisms to ensure that no more than one concurrent thread/process can process and execute a particular program portion at a time; this portion is known as the **critical section**. Synchronization ensures that only one thread at a time can access critical sections of code or shared data, preventing race conditions and inconsistent results.\n",
    "\n",
    "In a given program, when a thread is accessing/executing the critical section of the program, the other threads have to wait until that thread finishes executing. The typical goal of thread synchronization is to avoid any potential data discrepancies / race conditions when multiple threads access their shared resources, **allowing only one thread to execute the critical section of the program at a time** guarantees that no data conflicts occur in multithreaded applications.\n",
    "\n",
    "Let's discuss some of the synchronization mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449f594",
   "metadata": {},
   "source": [
    "#### 1. Lock / mutual exclusion (mutex)\n",
    "\n",
    "One of the most common ways to apply thread synchronization is through the implementation of a locking mechanism. In the `threading` module, the `threading.Lock` class provides a simple and intuitive approach to creating and working with locks. Its main usage includes the following methods: \n",
    "\n",
    "- `threading.Lock()`: This method initializes and returns a new lock object.\n",
    "- `acquire(blocking)`: When this method is called, all of the threads will run synchronously (that is, only one thread can execute the critical section at a time). The optional argument blocking allows us to specify whether the current thread should wait to acquire the lock:\n",
    "  - When `blocking = 0`, the current thread does not wait for the lock and simply returns 0 if the lock cannot be acquired by the thread, or 1 otherwise\n",
    "  - When `blocking = 1` (default value), the current thread blocks and waits for the lock to be released and acquires it afterwards\n",
    "- `release()`: When this method is called, the lock is released.\n",
    "\n",
    "Common pattern:\n",
    "\n",
    "```python\n",
    "import threading\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "try:\n",
    "    # Acquire the lock\n",
    "    lock.acquire()\n",
    "    \n",
    "    # Critical section - only one thread at a time can execute this code\n",
    "    critical_section_code()\n",
    "finally:\n",
    "    # Always release the lock, even if an exception occurs\n",
    "    lock.release()\n",
    "```\n",
    "\n",
    "Lock implements context manager protocol, so it is better practice to use `with` statement:\n",
    "\n",
    "```python\n",
    "import threading\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "with lock: # Automatically acquires and releases the lock \n",
    "    critical_region_code() # Critical section - only one thread at a time can execute this code\n",
    "```\n",
    "\n",
    "**See practical example 5**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3aef9d",
   "metadata": {},
   "source": [
    "#### 2. Semaphore\n",
    "\n",
    "This is one of the oldest synchronization primitives in the history of computer science, invented by the early Dutch computer scientist Edsger W. Dijkstra.\n",
    "\n",
    "A semaphore manages an internal counter which is decremented by each `acquire()` call and incremented by each `release()` call. The counter can never go below zero; when `acquire()` finds that it is zero, it blocks, waiting until some other thread calls `release()`.\n",
    "\n",
    "Use cases:\n",
    "\n",
    "- **Resource pool**: Allow only 5 simultaneous DB connections\n",
    "- **Rate limiting**: Max 2 API calls at once\n",
    "- **Thread-safe batching**: Limit how many threads can download files at once\n",
    "\n",
    "The `threading` module provides `threading.Semaphore` class for managing semaphores.\n",
    "\n",
    "**See practical example 6**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd3079f",
   "metadata": {},
   "source": [
    "#### 3. Event\n",
    "\n",
    "Used to signal between threads — one thread waits, another signals.\n",
    "\n",
    "An event object manages an internal flag that can be set to true with the `set()` method and reset to false with the `clear()` method. The `wait()` method blocks until the flag is true.\n",
    "\n",
    "The `threading` module provides `threading.Event`.\n",
    "\n",
    "**See practical example 7**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d54ba",
   "metadata": {},
   "source": [
    "#### 4. Queue\n",
    "\n",
    "A concept in computer science that is widely used in concurrent programming is queuing. **Queue** is a data structure that is a collection of different elements. Elements can be added to the end of the queue which is called enqueuing. Elements can be removed from the beginning of the queue, called dequeuing. It works in First in First out (FIFO) manner, meaning that first entered element is removed first. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/52/Data_Queue.svg/1200px-Data_Queue.svg.png\" alt=\"Index\" width=\"400\" height=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "The `queue` module in Python provides a simple implementation of the queue data structure. Each queue in the `queue.Queue` class can hold a specific amount of elements, and can have the following methods as its high-level API:\n",
    "- `get()`: This method returns the next element of the calling queue object and removes it from the queue object. If optional args `block` is `True` (the default) and `timeout` is `None` (the default), block if necessary until an item is available. If `timeout` is a positive number, it blocks at most `timeout` seconds and raises the `queue.Empty` exception if no item was available within that time.\n",
    "- `get_nowait()`: This method is equivalent to `get()` with `block` parameter set to `False`, meaning it returns an item if one is immediately available, else raises the `queue.Empty` exception .\n",
    "- `put()`: This method adds a new element to the calling queue object \n",
    "- `qsize()`: This method returns the number of current elements in the calling queue object (that is, its size)\n",
    "- `empty()`: This method returns a boolean, indicating whether the calling queue object is empty\n",
    "- `full()`: This method returns a boolean, indicating whether the calling queue object is full\n",
    "- `task_done()`: This method indicates that a formerly enqueued task is complete. Used by queue consumer threads. For each `get()` used to fetch a task, a subsequent call to `task_done()` tells the queue that the processing on the task is complete.\n",
    "- `join()`: This method blocks until all items in the queue have been gotten and processed. The count of unfinished tasks goes up whenever an item is added to the queue. The count goes down whenever a consumer thread calls `task_done()` to indicate that the item was retrieved and all work on it is complete. When the count of unfinished tasks drops to zero, `join()` unblocks.\n",
    "\n",
    "Sometimes it is undesirable to have as many threads as the tasks we have to process. Say we have a large number of tasks to be processed, then it will be quite inefficient to spawn the same large number of threads and have each thread execute only one task. It could be more beneficial to have a **fixed number of threads (commonly known as a thread pool)** that would work through the tasks in a cooperative manner.\n",
    "\n",
    "Here is when the concept of a queue comes in. We can design a structure in which the pool of threads will not hold any information regarding the tasks they should each execute, instead the tasks are stored in a queue (in other words task queue), and the items in the queue will be fed to individual members of the thread pool. As a given task is completed by a member of the thread pool, if the task queue still contains elements to be processed, then the next element in the queue will be sent to the thread that just became available.\n",
    "\n",
    "**See practical example 8**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b775d158",
   "metadata": {},
   "source": [
    "## IO-bound task\n",
    "\n",
    "In Python, I/O‑bound tasks are best handled with `threading` because most I/O operations (disk reads/writes, network calls, database queries) release the GIL while they’re waiting, you can get real concurrency with threads. Here’s how it compares:\n",
    "\n",
    "- **Threading**  \n",
    "  - **GIL release**: Most blocking I/O operations in the standard library (e.g., `socket`, file I/O, many database drivers) relinquish the GIL while waiting, so another thread can run.  \n",
    "  - **Low overhead**: Threads share memory and have much lower start‑up and context‑switch costs than processes.  \n",
    "  - **Ease of sharing state**: Because threads live in the same address space, shared data structures require only locks or queues, not inter-process communication (IPC).  \n",
    "  - **Good for high‑latency tasks**: If your workload is dominated by waiting for network responses, disk reads, or other slow operations, threading can boost throughput without much complexity.\n",
    "\n",
    "- **Multiprocessing**  \n",
    "  - **Still works**: Processes also release the GIL on blocking I/O, but using multiple processes adds IPC overhead and higher memory usage.  \n",
    "  - **Overkill for I/O**: Spinning up separate interpreters usually isn’t worth it if you’re not CPU‑bound; threads are lighter and simpler.\n",
    "\n",
    "- **Asynchronous I/O (async/await)**  \n",
    "  - **Single-threaded concurrency**: Uses an event loop to switch between tasks when they await I/O.  \n",
    "  - **Even lighter than threads**: No context‑switching at the OS level; perfect for handling *huge* numbers of concurrent connections (web servers, chat clients).  \n",
    "  - **Requires non‑blocking APIs**: You need libraries designed for asyncio or third‑party frameworks (e.g., `aiohttp`, `asyncpg`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b201a9",
   "metadata": {},
   "source": [
    "### Simple web scraper\n",
    "\n",
    "To demonstrate the difference in execution time between sequential and multithreaded approaches, we'll simulate downloading content from multiple URLs.\n",
    "\n",
    "Requesting a content over a network is I/O-bound task and well-suited for multi-threading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6469ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c112b7",
   "metadata": {},
   "source": [
    "In the following examples, we use sequential and multi-threading techniques to download websites. Also, we can do it with multi-processing, but since this is a I/O-bound task, it is better to solve the problem with multi-threading. Multi-processing can create additional overhead.\n",
    "\n",
    "**See practical example 9**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c3bb6f",
   "metadata": {},
   "source": [
    "## Python experimental support for free threading\n",
    "\n",
    "Starting with version `3.13`, CPython introduces an experimental feature called free threading, which removes the Global Interpreter Lock (GIL). This change enables true parallel execution of threads across multiple CPU cores, allowing programs that are designed to use threading to take full advantage of multi-core systems and potentially see performance improvements.\n",
    "\n",
    "However, this mode is still in development and may contain bugs. Additionally, programs that run in a single thread may experience a noticeable performance decrease.\n",
    "\n",
    "This was introduced in [PEP 703 – Making the Global Interpreter Lock Optional in CPython](https://peps.python.org/pep-0703/). See [documentation](https://docs.python.org/3/howto/free-threading-python.html) for installation guides and more information.\n",
    "\n",
    "**See practical example 10**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8778d4f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [threading — Thread-based parallelism](https://docs.python.org/3/library/threading.html)\n",
    "- [Python experimental support for free threading](https://docs.python.org/3/howto/free-threading-python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d4bc3",
   "metadata": {},
   "source": [
    "# Multi-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edd2606",
   "metadata": {},
   "source": [
    "## `multiprocessing` module\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `multiprocessing` module provides a way to create new processes using an API similar to the `threading` module. Unlike threads, it uses separate subprocesses, which allows it to bypass the Global Interpreter Lock (GIL) and take full advantage of multiple CPU cores. This enables true parallel execution and works on both POSIX systems and Windows.\n",
    "\n",
    "In addition to thread-like functionality, `multiprocessing` includes features not found in the `threading` module. One notable feature is the `Pool` class, which simplifies running a function in parallel across a collection of inputs—this is known as data parallelism.\n",
    "\n",
    "`multiprocessing.Process` objects represent activity that is run in a separate process. The `multiprocessing.Process` class has equivalents of all the methods of `threading.Thread`.\n",
    "\n",
    "The `if __name__ == '__main__'` part is necessary in multi-processing as you can see in the following example. This is to make sure that the main module can be safely imported by a new Python interpreter without causing unintended side effects (such as starting a new process).\n",
    "\n",
    "**See practical example 11**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4b1c6",
   "metadata": {},
   "source": [
    "### Contexts and start methods\n",
    "\n",
    "#### 1. **`spawn`**\n",
    "- **How it works:** Starts a *fresh Python interpreter process*.\n",
    "- **Pros:** Clean slate—only essential resources are inherited.\n",
    "- **Cons:** Slower startup.\n",
    "- **Default on:** **Windows** and **macOS**.\n",
    "\n",
    "#### 2. **`fork`**\n",
    "- **How it works:** Uses `os.fork()`. Child is a clone of the parent.\n",
    "- **Pros:** Very fast.\n",
    "- **Cons:** Not safe with threads. Can crash on **macOS**.\n",
    "- **Default on:** Most **Linux**/POSIX systems (but **changing in Python 3.14**).\n",
    "- **Deprecated** for multi-threaded environments since Python 3.12 because forking a multi-threaded process is problematic.\n",
    "\n",
    "#### 3. **`forkserver`**\n",
    "- **How it works:** Starts a **server** process which handles forking. As it is single-threaded, it is safer to use `fork` method.\n",
    "- **Pros:** Safer than `fork`, faster than `spawn`. No excess resources inherited.\n",
    "- **Cons:** Requires OS support for file descriptor passing.\n",
    "- **Available on:** POSIX with certain features (e.g., Linux).\n",
    "\n",
    "Here are two ways to select a start method.\n",
    "\n",
    "#### Option 1: `set_start_method()` (set once per program)\n",
    "\n",
    "```python\n",
    "import multiprocessing as mp\n",
    "\n",
    "def foo(q):\n",
    "    q.put('hello')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.set_start_method('spawn')  # 'spawn', 'fork', or 'forkserver'\n",
    "    q = mp.Queue()\n",
    "    p = mp.Process(target=foo, args=(q,))\n",
    "    p.start()\n",
    "    print(q.get())\n",
    "    p.join()\n",
    "```\n",
    "\n",
    "#### Option 2: `get_context()` (preferred for libraries or multiple modes)\n",
    "\n",
    "This avoids conflicts with other parts of the app or external libraries.\n",
    "\n",
    "```python\n",
    "import multiprocessing as mp\n",
    "\n",
    "def foo(q):\n",
    "    q.put('hello')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ctx = mp.get_context('spawn')\n",
    "    q = ctx.Queue()\n",
    "    p = ctx.Process(target=foo, args=(q,))\n",
    "    p.start()\n",
    "    print(q.get())\n",
    "    p.join()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31660459",
   "metadata": {},
   "source": [
    "## Inter-Process Communication (IPC)\n",
    "\n",
    "Inter-Process Communication (IPC) refers to mechanisms that allow processes to exchange data and coordinate their actions. These mechanisms are essential for building complex systems where multiple processes need to work together. Synchronization, shared memory, queues and pipes are some examples for organizing IPC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb816a",
   "metadata": {},
   "source": [
    "### Synchronization between processes\n",
    "\n",
    "The `multiprocessing` module offers the same synchronization tools as the `threading` module. For example, you can use a **lock** to prevent multiple processes from writing to the console at the same time, which avoids jumbled output.\n",
    "\n",
    "Here's a version of that concept using `for` loop syntax with a lock:\n",
    "\n",
    "```python\n",
    "from multiprocessing import Process, Lock\n",
    "\n",
    "\n",
    "lock = Lock()\n",
    "\n",
    "\n",
    "def safe_print(number):\n",
    "    with lock:\n",
    "        print('hello world', number)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for i in range(10):\n",
    "        p = Process(target=safe_print, args=(i,))\n",
    "        p.start()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edbc310",
   "metadata": {},
   "source": [
    "# Exchanging objects between processes\n",
    "\n",
    "When using multiple processes, one generally uses message passing for communication between processes and avoids having to use any synchronization primitives like locks.\n",
    "\n",
    "For passing messages one can use `multiprocessing.Pipe()` (for a connection between two processes) or a queue (which allows multiple producers and consumers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e22b5e",
   "metadata": {},
   "source": [
    "### `multiprocessing.Pipe([duplex])`\n",
    "\n",
    "Returns a pair `(conn1, conn2)` of `multiprocessing.connection.Connection` objects representing the ends of a pipe.\n",
    "\n",
    "If `duplex` is `True` (the default) then the pipe is bidirectional. If `duplex` is `False` then the pipe is unidirectional: `conn1` can only be used for receiving messages and` conn2` can only be used for sending messages.\n",
    "\n",
    "Note that data in a pipe may become corrupted if two processes (or threads) try to read from or write to the same end of the pipe at the same time. Of course there is no risk of corruption from processes using different ends of the pipe at the same time.\n",
    "\n",
    "The `send()` method serializes the object using `pickle` and the `recv()` re-creates the object.\n",
    "\n",
    "#### Key Characteristics\n",
    "\n",
    "- **Byte Stream:** Pipes typically handle an unstructured stream of bytes. The writing process sends bytes, and the reading process receives bytes, often without inherent message boundaries. The reader needs to know how to interpret the byte stream (e.g., reading until a newline character or reading a fixed number of bytes).\n",
    "- **Kernel-Managed Buffer:** The operating system manages a buffer for the pipe. If the writer produces data faster than the reader consumes it, the data accumulates in the buffer. If the buffer fills up, the writer will block (wait) until the reader consumes some data. Conversely, if the reader tries to read from an empty pipe, it will block until the writer sends data.\n",
    "- **Synchronization:** The blocking behaviour provides implicit synchronization between the producer (writer) and consumer (reader).\n",
    "\n",
    "**See practical example 12**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaaaa6c",
   "metadata": {},
   "source": [
    "### Queues\n",
    "\n",
    "- Similar to pipes, these are another mechanism for Inter-Process Communication (IPC).\n",
    "- **Message-Oriented:** Unlike pipes which handle byte streams, message queues typically handle discrete messages. The sender enqueues a whole message, and the receiver dequeues a whole message. This preserves message boundaries.\n",
    "- **Many-to-Many:** Often, multiple processes can write to the same queue, and multiple processes can read from it (though often a message is consumed by only one reader).\n",
    "\n",
    "In Python, the `multiprocessing.Queue`, `multiprocessing.SimpleQueue` and `multiprocessing.JoinableQueue` types are multi-producer, multi-consumer FIFO queues modelled on the `queue.Queue` class in the standard library. They differ in that `multiprocessing.Queue` lacks the `task_done()` and `join()` methods introduced into Python 2.5’s `queue.Queue` class.\n",
    "\n",
    "If you use `JoinableQueue` then you must call `JoinableQueue.task_done()` for each task removed from the queue or else the semaphore used to count the number of unfinished tasks may eventually overflow, raising an exception.\n",
    "\n",
    "One difference from other Python queue implementations, is that `multiprocessing` queues serializes all objects that are put into them using `pickle`. The object return by the `get` method is a re-created object that does not share memory with the original object.\n",
    "\n",
    "Multi-processing queues are thread and process safe.\n",
    "\n",
    "**See practical example 13**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
